{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAZRIRRLxyCeULwPOssBdg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abidjahan1/sentiment_anlaysis/blob/main/logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vBVWeO166JUM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v7q0Jr-K6IwQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Oik4Y9em1CU8"
      },
      "outputs": [],
      "source": [
        "# !pip install kagglehub[pandas-datasets]\n",
        "\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title dataset uploading with kaggle API\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !mv kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "XVWSilux6KrP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "\n",
        "# Download the dataset using kagglehub\n",
        "path = kagglehub.dataset_download(\"kazanova/sentiment140\")\n",
        "\n",
        "print(\"Dataset files downloaded to:\", path)\n",
        "\n",
        "# Construct the file path to the CSV\n",
        "csv_path = f\"{path}/training.1600000.processed.noemoticon.csv\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGf9Mq4p6PV3",
        "outputId": "05f74862-8889-4622-84bb-fdc7def4f368"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset files downloaded to: /kaggle/input/sentiment140\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load csv file with custom names\n",
        "\n",
        "# Load CSV with custom column names\n",
        "columns = ['target', 'ids', 'date', 'flag', 'user_name', 'text']\n",
        "\n",
        "# Load CSV with pandas\n",
        "df = pd.read_csv(csv_path, encoding=\"latin-1\", header=None,names = columns)\n",
        "df.head(50)"
      ],
      "metadata": {
        "id": "7oM--siM9YR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #@title Filter and Clean Data\n",
        "\n",
        " # Keep only positive (4) and negative (0) sentiments\n",
        "df = df[df['target'].isin([0, 4])]\n",
        "\n",
        "# Map sentiment values: 0 → 0 (negative), 4 → 1 (positive)\n",
        "df['target'] = df['target'].map({0: 0, 4: 1})\n",
        "\n",
        "# Drop unused columns\n",
        "df = df[['text', 'target']]\n",
        "\n",
        "# Check distribution\n",
        "df['target'].value_counts()\n"
      ],
      "metadata": {
        "id": "qJwCyvZl9oSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preprocess Text\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()                          # Lowercase all text\n",
        "    text = re.sub(r'http\\S+', '', text)          # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)             # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)             # Remove hashtags\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)         # Remove punctuation/numbers\n",
        "    return text.strip()\n",
        "\n",
        "# Apply cleaning to all tweets\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "wPFhapwz-r-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Better Text Preprocessing\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]  # remove stopwords\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Apply cleaning to all tweets\n",
        "df['text'] = df['text'].apply(clean_text)\n",
        "\n",
        "df['text']"
      ],
      "metadata": {
        "id": "AUMPpNvNCqB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Split into Train and Test Sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df['text']\n",
        "y = df['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_test, y_train, y_test\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sQWkfbTm_D_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Text Vectorization with TF-IDF\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create the vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "\n",
        "# Fit on training data and transform both sets\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "'''\n",
        "TF-IDF converts text into numeric features by calculating:\n",
        "\n",
        "Term Frequency (TF)\n",
        "\n",
        "Inverse Document Frequency (IDF)\n",
        "\n",
        "max_features=5000: keeps the top 5000 most important words.\n",
        "Adds word pairs (bigrams) like not good, which help detect sarcasm or sentiment more accurately.\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "BqCMsscI_l_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Try Class Balancing\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate classes\n",
        "df_pos = df[df['target'] == 1]\n",
        "df_neg = df[df['target'] == 0]\n",
        "\n",
        "# Downsample majority to match minority\n",
        "df_pos_down = resample(df_pos, replace=False, n_samples=len(df_neg), random_state=42)\n",
        "\n",
        "# Combine again\n",
        "df_balanced = pd.concat([df_neg, df_pos_down])\n"
      ],
      "metadata": {
        "id": "qgr4aBUaEacC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Train Logistic Regression Model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Train (fit) the model\n",
        "model.fit(X_train_vec, y_train)\n",
        "\n",
        "'''Logistic Regression is a linear model for binary classification.\n",
        "\n",
        "fit() tells the model to learn the relationship between tweet vectors and sentiment labels.'''"
      ],
      "metadata": {
        "id": "AwdZMPN7AgwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title  Evaluate the Model\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Evaluate accuracy and metrics\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "\n",
        "\"\"\"\n",
        "Accuracy: 0.77364375\n",
        "\n",
        "Classification Report:\n",
        "               precision    recall  f1-score   support\n",
        "\n",
        "           0       0.79      0.75      0.77    159494\n",
        "           1       0.76      0.80      0.78    160506\n",
        "\n",
        "    accuracy                           0.77    320000\n",
        "   macro avg       0.77      0.77      0.77    320000\n",
        "weighted avg       0.77      0.77      0.77    320000\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "xMO5lLU0BKGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Save Model and Vectorizer for Django\n",
        "import pickle\n",
        "\n",
        "# Save model\n",
        "with open('model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)\n",
        "\n",
        "# Save TF-IDF vectorizer\n",
        "with open('tfidf.pkl', 'wb') as f:\n",
        "    pickle.dump(vectorizer, f)\n",
        "\n",
        "\n",
        "'''\n",
        "pickle  save Python objects to reuse later (in Django).\n",
        "\n",
        " load model.pkl inside Django to use the trained model.\n",
        "'''"
      ],
      "metadata": {
        "id": "T4dGZjruBWR2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}